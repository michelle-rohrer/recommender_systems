{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems - Mini Challenge HS25\n",
    "\n",
    "In this minichallenge we will explore a MovieLens dataset and implement several recommender systems and evaluation methods. Subsequently we will optimize these methods and compare the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submission deadline:** Sunday of SW11 um 18:00 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guidelines for Implementation and Submission\n",
    "- Code must be written in Python. The versions of all used packages must be given for reproducability.\n",
    "- You may respond in English or German.\n",
    "- We develop numerous algorithms ourselves. Unless explicitly stated otherwise, only the following libraries may be used in Python: numpy, matplotlib, seaborn, pandas. \n",
    "- Follow good coding practices and write modular, reusable code.\n",
    "- The submitted solution must contain all codes and the results. No code may be outsourced.\n",
    "- All pathes must be relative and just downloading your repo must be executable without modifications.\n",
    "- Only fully running code is graded. The notebook must run sequential from start to end.\n",
    "- During development, if computation time is too long for productive prototyping and debugging work, it is recommended to reduce the dataset to a fraction of its original. However, final results must be calculated on the full dataset. \n",
    "- All plots must be fully labeled (title, axes, labels, colorbar, etc.) so that the plot can be easily understood.\n",
    "- Each plot must be accompanied by a brief discussion, which explains the plot and captures the key insights that become visible.\n",
    "- Only fully labeled plots with an accompanying discussion will be assessed.\n",
    "- The last commit in your fork of the repo before the submission deadline counts as the submission.\n",
    "- Points will be deducted if you write inconsise (Denial of service will be punished) or if I read text not written for me but for the user of ChatGPT oir similar. \n",
    "- If you would like to submit and have the mini-challenge assessed, please send a short email to the subject expert (moritz.kirschmann@fhnw.ch) within 2 days after submission.\n",
    "- Please do not delete, duplicate, or move the existing cells. This leads to problems during the correction. However, you may add as many additional cells as you like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 - A deep exploration of the dataset (17 points)\n",
    "We will work with a subset of the MovieLens dataset. This subset is located under ``data/ml-latest-small``. Read the ``README.txt``carefully. \n",
    "Open the files. \n",
    "\n",
    "a) Describe the available data.\n",
    "\n",
    "b) Find and fix bad data (e.g. duplicates, missing values, etc.).\n",
    "\n",
    "Generate lists of\n",
    "\n",
    "c) - Top 20 movies by average rating\n",
    "\n",
    "d) - Top 20 movies by number of views\n",
    "\n",
    "e) What is the range of the ratings? \n",
    "\n",
    "f) Which genre has be rated how many times?\n",
    "\n",
    "g) How sparse is the User Rating Matrix?\n",
    "\n",
    "Plot the following:\n",
    "\n",
    "h) How many users have rated how many movies\n",
    "\n",
    "i) Which rating is given how often over time with a time resolution of month \n",
    "\n",
    "j) Which rating is given how often per genre\n",
    "\n",
    "k) The rating distributions of 10 random movies\n",
    "\n",
    "l) The rating distributions of 3 movies that you have watched\n",
    "\n",
    "m) How many users give which average rating\n",
    "\n",
    "n) How often a movie was rated as a function of average rating\n",
    "\n",
    "o) A heatmap of the User Item Matrix\n",
    "\n",
    "p) A heatmap of the User Item Matrix for the 100 most rated movies for the 50 users with most ratings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1 - A deep exploration of the dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Load the datasets\n",
    "print(\"Loading datasets...\")\n",
    "ratings = pd.read_csv('data/ratings.csv')\n",
    "movies = pd.read_csv('data/movies.csv')\n",
    "links = pd.read_csv('data/links.csv')\n",
    "tags = pd.read_csv('data/tags.csv')\n",
    "\n",
    "print(\"Data loaded successfully!\")\n",
    "print(\"\\nDataset shapes:\")\n",
    "print(f\"Ratings: {ratings.shape}\")\n",
    "print(f\"Movies: {movies.shape}\")\n",
    "print(f\"Links: {links.shape}\")\n",
    "print(f\"Tags: {tags.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) Describe the available data\n",
    "\n",
    "print(\"=== DATASET DESCRIPTION ===\")\n",
    "print(\"\\n1. RATINGS DATASET:\")\n",
    "print(ratings.head())\n",
    "print(f\"\\nColumns: {list(ratings.columns)}\")\n",
    "print(f\"Data types:\\n{ratings.dtypes}\")\n",
    "print(f\"Basic statistics:\\n{ratings.describe()}\")\n",
    "\n",
    "print(\"\\n2. MOVIES DATASET:\")\n",
    "print(movies.head())\n",
    "print(f\"\\nColumns: {list(movies.columns)}\")\n",
    "print(f\"Data types:\\n{movies.dtypes}\")\n",
    "\n",
    "print(\"\\n3. LINKS DATASET:\")\n",
    "print(links.head())\n",
    "print(f\"\\nColumns: {list(links.columns)}\")\n",
    "print(f\"Data types:\\n{links.dtypes}\")\n",
    "\n",
    "print(\"\\n4. TAGS DATASET:\")\n",
    "print(tags.head())\n",
    "print(f\"\\nColumns: {list(tags.columns)}\")\n",
    "print(f\"Data types:\\n{tags.dtypes}\")\n",
    "\n",
    "print(\"\\n=== DATA OVERVIEW ===\")\n",
    "print(f\"• Total ratings: {len(ratings):,}\")\n",
    "print(f\"• Unique users: {ratings['userId'].nunique():,}\")\n",
    "print(f\"• Unique movies: {ratings['movieId'].nunique():,}\")\n",
    "print(f\"• Total movies in dataset: {len(movies):,}\")\n",
    "print(f\"• Total tags: {len(tags):,}\")\n",
    "print(f\"• Rating period: {datetime.fromtimestamp(ratings['timestamp'].min()).strftime('%Y-%m-%d')} to {datetime.fromtimestamp(ratings['timestamp'].max()).strftime('%Y-%m-%d')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) Find and fix bad data (duplicates, missing values, etc.)\n",
    "\n",
    "print(\"=== DATA QUALITY CHECK ===\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n1. MISSING VALUES:\")\n",
    "print(\"Ratings dataset:\")\n",
    "print(ratings.isnull().sum())\n",
    "print(\"\\nMovies dataset:\")\n",
    "print(movies.isnull().sum())\n",
    "print(\"\\nLinks dataset:\")\n",
    "print(links.isnull().sum())\n",
    "print(\"\\nTags dataset:\")\n",
    "print(tags.isnull().sum())\n",
    "\n",
    "# Check for duplicates\n",
    "print(\"\\n2. DUPLICATES:\")\n",
    "print(f\"Duplicate ratings: {ratings.duplicated().sum()}\")\n",
    "print(f\"Duplicate movies: {movies.duplicated().sum()}\")\n",
    "print(f\"Duplicate links: {links.duplicated().sum()}\")\n",
    "print(f\"Duplicate tags: {tags.duplicated().sum()}\")\n",
    "\n",
    "# Check for rating duplicates (same user rating same movie multiple times)\n",
    "print(f\"\\nDuplicate user-movie ratings: {ratings.duplicated(subset=['userId', 'movieId']).sum()}\")\n",
    "\n",
    "# Check data consistency\n",
    "print(\"\\n3. DATA CONSISTENCY:\")\n",
    "print(f\"Movies in ratings but not in movies: {set(ratings['movieId']) - set(movies['movieId'])}\")\n",
    "print(f\"Movies in movies but not in ratings: {len(set(movies['movieId']) - set(ratings['movieId']))}\")\n",
    "\n",
    "# Check for invalid ratings\n",
    "print(\"\\n4. INVALID RATINGS:\")\n",
    "invalid_ratings = ratings[(ratings['rating'] < 0.5) | (ratings['rating'] > 5.0)]\n",
    "print(f\"Invalid ratings (outside 0.5-5.0 range): {len(invalid_ratings)}\")\n",
    "\n",
    "# Check for movies with no genres\n",
    "print(f\"\\nMovies with no genres: {movies[movies['genres'] == '(no genres listed)'].shape[0]}\")\n",
    "\n",
    "print(\"\\n=== DATA CLEANING ===\")\n",
    "# Remove duplicates if any\n",
    "if ratings.duplicated(subset=['userId', 'movieId']).sum() > 0:\n",
    "    print(\"Removing duplicate user-movie ratings...\")\n",
    "    ratings = ratings.drop_duplicates(subset=['userId', 'movieId'])\n",
    "    print(f\"Ratings after removing duplicates: {len(ratings)}\")\n",
    "\n",
    "# Convert timestamp to datetime for easier analysis\n",
    "ratings['datetime'] = pd.to_datetime(ratings['timestamp'], unit='s')\n",
    "tags['datetime'] = pd.to_datetime(tags['timestamp'], unit='s')\n",
    "\n",
    "print(\"Data cleaning completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c) Top 20 movies by average rating\n",
    "\n",
    "# Merge ratings with movies to get movie titles\n",
    "movie_stats = ratings.groupby('movieId').agg({\n",
    "    'rating': ['mean', 'count'],\n",
    "    'userId': 'count'\n",
    "}).round(3)\n",
    "\n",
    "movie_stats.columns = ['avg_rating', 'rating_count', 'user_count']\n",
    "movie_stats = movie_stats.reset_index()\n",
    "\n",
    "# Merge with movie information\n",
    "movie_stats = movie_stats.merge(movies[['movieId', 'title', 'genres']], on='movieId')\n",
    "\n",
    "# Filter movies with at least 50 ratings to avoid bias from movies with very few ratings\n",
    "min_ratings = 50\n",
    "top_movies_by_rating = movie_stats[movie_stats['rating_count'] >= min_ratings].sort_values('avg_rating', ascending=False).head(20)\n",
    "\n",
    "print(\"=== TOP 20 MOVIES BY AVERAGE RATING (min 50 ratings) ===\")\n",
    "for idx, row in top_movies_by_rating.iterrows():\n",
    "    print(f\"{row['avg_rating']:.3f} - {row['title']} ({row['rating_count']} ratings)\")\n",
    "\n",
    "print(f\"\\nNote: Only movies with at least {min_ratings} ratings are included to avoid bias from movies with very few ratings.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d) Top 20 movies by number of views (ratings)\n",
    "\n",
    "top_movies_by_views = movie_stats.sort_values('rating_count', ascending=False).head(20)\n",
    "\n",
    "print(\"=== TOP 20 MOVIES BY NUMBER OF RATINGS ===\")\n",
    "for idx, row in top_movies_by_views.iterrows():\n",
    "    print(f\"{row['rating_count']} ratings - {row['title']} (avg: {row['avg_rating']:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e) What is the range of the ratings?\n",
    "\n",
    "print(\"=== RATING RANGE ANALYSIS ===\")\n",
    "print(f\"Minimum rating: {ratings['rating'].min()}\")\n",
    "print(f\"Maximum rating: {ratings['rating'].max()}\")\n",
    "print(f\"Rating range: {ratings['rating'].min()} to {ratings['rating'].max()}\")\n",
    "print(f\"Unique rating values: {sorted(ratings['rating'].unique())}\")\n",
    "print(f\"Number of unique rating values: {ratings['rating'].nunique()}\")\n",
    "\n",
    "# Rating distribution\n",
    "print(\"\\nRating distribution:\")\n",
    "rating_dist = ratings['rating'].value_counts().sort_index()\n",
    "for rating, count in rating_dist.items():\n",
    "    percentage = (count / len(ratings)) * 100\n",
    "    print(f\"Rating {rating}: {count:,} ratings ({percentage:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f) Which genre has been rated how many times?\n",
    "\n",
    "# First, let's expand the genres (pipe-separated) into individual rows\n",
    "movie_genres = movies.copy()\n",
    "movie_genres['genres_list'] = movie_genres['genres'].str.split('|')\n",
    "movie_genres = movie_genres.explode('genres_list')\n",
    "movie_genres['genre'] = movie_genres['genres_list']\n",
    "\n",
    "# Merge with ratings to get rating counts per genre\n",
    "genre_ratings = ratings.merge(movie_genres[['movieId', 'genre']], on='movieId')\n",
    "\n",
    "# Count ratings per genre\n",
    "genre_stats = genre_ratings.groupby('genre').agg({\n",
    "    'rating': ['count', 'mean'],\n",
    "    'userId': 'nunique'\n",
    "}).round(3)\n",
    "\n",
    "genre_stats.columns = ['total_ratings', 'avg_rating', 'unique_users']\n",
    "genre_stats = genre_stats.sort_values('total_ratings', ascending=False)\n",
    "\n",
    "print(\"=== GENRE RATING STATISTICS ===\")\n",
    "print(f\"{'Genre':<20} {'Total Ratings':<15} {'Avg Rating':<12} {'Unique Users':<15}\")\n",
    "print(\"-\" * 65)\n",
    "for genre, row in genre_stats.iterrows():\n",
    "    print(f\"{genre:<20} {row['total_ratings']:<15,} {row['avg_rating']:<12.3f} {row['unique_users']:<15,}\")\n",
    "\n",
    "print(f\"\\nTotal ratings analyzed: {genre_stats['total_ratings'].sum():,}\")\n",
    "print(f\"Note: Some ratings may be counted multiple times if a movie has multiple genres.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g) How sparse is the User Rating Matrix?\n",
    "\n",
    "# Create user-item matrix to analyze sparsity\n",
    "user_item_matrix = ratings.pivot_table(index='userId', columns='movieId', values='rating')\n",
    "\n",
    "print(\"=== USER RATING MATRIX SPARSITY ANALYSIS ===\")\n",
    "print(f\"Matrix shape: {user_item_matrix.shape} (users x movies)\")\n",
    "print(f\"Total possible ratings: {user_item_matrix.shape[0] * user_item_matrix.shape[1]:,}\")\n",
    "print(f\"Actual ratings: {ratings.shape[0]:,}\")\n",
    "print(f\"Missing ratings: {(user_item_matrix.shape[0] * user_item_matrix.shape[1]) - ratings.shape[0]:,}\")\n",
    "\n",
    "# Calculate sparsity\n",
    "sparsity = 1 - (ratings.shape[0] / (user_item_matrix.shape[0] * user_item_matrix.shape[1]))\n",
    "print(f\"Sparsity: {sparsity:.4f} ({sparsity*100:.2f}%)\")\n",
    "print(f\"Density: {1-sparsity:.4f} ({(1-sparsity)*100:.2f}%)\")\n",
    "\n",
    "# Additional sparsity insights\n",
    "print(f\"\\nSparsity insights:\")\n",
    "print(f\"• Average ratings per user: {ratings.shape[0] / user_item_matrix.shape[0]:.1f}\")\n",
    "print(f\"• Average ratings per movie: {ratings.shape[0] / user_item_matrix.shape[1]:.1f}\")\n",
    "print(f\"• Users with most ratings: {ratings.groupby('userId').size().max()}\")\n",
    "print(f\"• Movies with most ratings: {ratings.groupby('movieId').size().max()}\")\n",
    "print(f\"• Users with fewest ratings: {ratings.groupby('userId').size().min()}\")\n",
    "print(f\"• Movies with fewest ratings: {ratings.groupby('movieId').size().min()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h) Plot: How many users have rated how many movies\n",
    "\n",
    "# Calculate ratings per user\n",
    "user_rating_counts = ratings.groupby('userId').size().sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot 1: Histogram of ratings per user\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(user_rating_counts, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Number of Movies Rated')\n",
    "plt.ylabel('Number of Users')\n",
    "plt.title('Distribution of Ratings per User')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Cumulative distribution\n",
    "plt.subplot(1, 2, 2)\n",
    "sorted_counts = user_rating_counts.sort_values(ascending=False)\n",
    "cumulative_users = np.arange(1, len(sorted_counts) + 1)\n",
    "plt.plot(sorted_counts.values, cumulative_users, linewidth=2, color='red')\n",
    "plt.xlabel('Number of Movies Rated')\n",
    "plt.ylabel('Cumulative Number of Users')\n",
    "plt.title('Cumulative Distribution of Ratings per User')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"User rating statistics:\")\n",
    "print(f\"• Min ratings per user: {user_rating_counts.min()}\")\n",
    "print(f\"• Max ratings per user: {user_rating_counts.max()}\")\n",
    "print(f\"• Mean ratings per user: {user_rating_counts.mean():.1f}\")\n",
    "print(f\"• Median ratings per user: {user_rating_counts.median():.1f}\")\n",
    "print(f\"• Std ratings per user: {user_rating_counts.std():.1f}\")\n",
    "\n",
    "print(f\"\\nTop 10 users by number of ratings:\")\n",
    "for i, (user_id, count) in enumerate(user_rating_counts.head(10).items(), 1):\n",
    "    print(f\"{i:2d}. User {user_id}: {count} ratings\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i) Plot: Which rating is given how often over time with monthly resolution\n",
    "\n",
    "# Add year-month column for time analysis\n",
    "ratings['year_month'] = ratings['datetime'].dt.to_period('M')\n",
    "\n",
    "# Count ratings by month and rating value\n",
    "monthly_ratings = ratings.groupby(['year_month', 'rating']).size().unstack(fill_value=0)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Stacked area chart of ratings over time\n",
    "plt.subplot(2, 1, 1)\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(monthly_ratings.columns)))\n",
    "monthly_ratings.plot(kind='area', stacked=True, color=colors, alpha=0.7, figsize=(15, 8))\n",
    "plt.title('Rating Distribution Over Time (Monthly)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Time (Year-Month)')\n",
    "plt.ylabel('Number of Ratings')\n",
    "plt.legend(title='Rating Value', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Line plot showing trends for each rating\n",
    "plt.subplot(2, 1, 2)\n",
    "for rating_val in sorted(monthly_ratings.columns):\n",
    "    plt.plot(monthly_ratings.index.astype(str), monthly_ratings[rating_val], \n",
    "             marker='o', label=f'Rating {rating_val}', linewidth=2, markersize=4)\n",
    "plt.title('Rating Trends Over Time', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Time (Year-Month)')\n",
    "plt.ylabel('Number of Ratings')\n",
    "plt.legend(title='Rating Value', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Monthly rating trends analysis:\")\n",
    "print(f\"• Dataset spans from {ratings['year_month'].min()} to {ratings['year_month'].max()}\")\n",
    "print(f\"• Total months with data: {ratings['year_month'].nunique()}\")\n",
    "print(f\"• Average ratings per month: {len(ratings) / ratings['year_month'].nunique():.1f}\")\n",
    "\n",
    "# Show some monthly statistics\n",
    "monthly_totals = monthly_ratings.sum(axis=1)\n",
    "print(f\"\\nTop 5 months with most ratings:\")\n",
    "for month, count in monthly_totals.nlargest(5).items():\n",
    "    print(f\"• {month}: {count:,} ratings\")\n",
    "\n",
    "print(f\"\\nTop 5 months with fewest ratings:\")\n",
    "for month, count in monthly_totals.nsmallest(5).items():\n",
    "    print(f\"• {month}: {count:,} ratings\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# j) Plot: Which rating is given how often per genre\n",
    "\n",
    "# Use the genre_ratings data we created earlier\n",
    "genre_rating_dist = genre_ratings.groupby(['genre', 'rating']).size().unstack(fill_value=0)\n",
    "\n",
    "# Calculate percentages for each genre\n",
    "genre_rating_pct = genre_rating_dist.div(genre_rating_dist.sum(axis=1), axis=0) * 100\n",
    "\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "# Plot 1: Stacked bar chart of absolute counts\n",
    "plt.subplot(2, 1, 1)\n",
    "genre_rating_dist.plot(kind='bar', stacked=True, figsize=(16, 8), \n",
    "                       color=plt.cm.viridis(np.linspace(0, 1, len(genre_rating_dist.columns))))\n",
    "plt.title('Rating Distribution by Genre (Absolute Counts)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Genre')\n",
    "plt.ylabel('Number of Ratings')\n",
    "plt.legend(title='Rating Value', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Stacked bar chart of percentages\n",
    "plt.subplot(2, 1, 2)\n",
    "genre_rating_pct.plot(kind='bar', stacked=True, figsize=(16, 8),\n",
    "                      color=plt.cm.viridis(np.linspace(0, 1, len(genre_rating_pct.columns))))\n",
    "plt.title('Rating Distribution by Genre (Percentages)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Genre')\n",
    "plt.ylabel('Percentage of Ratings')\n",
    "plt.legend(title='Rating Value', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Genre rating analysis:\")\n",
    "print(\"\\nAverage rating by genre:\")\n",
    "avg_rating_by_genre = genre_ratings.groupby('genre')['rating'].mean().sort_values(ascending=False)\n",
    "for genre, avg_rating in avg_rating_by_genre.items():\n",
    "    total_ratings = genre_rating_dist.loc[genre].sum()\n",
    "    print(f\"• {genre:<20}: {avg_rating:.3f} (from {total_ratings:,} ratings)\")\n",
    "\n",
    "print(f\"\\nMost common rating by genre:\")\n",
    "for genre in genre_rating_dist.index:\n",
    "    most_common_rating = genre_rating_dist.loc[genre].idxmax()\n",
    "    count = genre_rating_dist.loc[genre].max()\n",
    "    percentage = (count / genre_rating_dist.loc[genre].sum()) * 100\n",
    "    print(f\"• {genre:<20}: Rating {most_common_rating} ({count:,} ratings, {percentage:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k) Plot: Rating distributions of 10 random movies\n",
    "\n",
    "# Select 10 random movies that have at least 20 ratings for better visualization\n",
    "movies_with_sufficient_ratings = movie_stats[movie_stats['rating_count'] >= 20]['movieId'].tolist()\n",
    "random_movies = np.random.choice(movies_with_sufficient_ratings, size=10, replace=False)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, movie_id in enumerate(random_movies, 1):\n",
    "    plt.subplot(2, 5, i)\n",
    "    \n",
    "    # Get ratings for this movie\n",
    "    movie_ratings = ratings[ratings['movieId'] == movie_id]['rating']\n",
    "    \n",
    "    # Create histogram\n",
    "    plt.hist(movie_ratings, bins=np.arange(0.5, 6, 0.5), alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    \n",
    "    # Get movie title\n",
    "    movie_title = movies[movies['movieId'] == movie_id]['title'].iloc[0]\n",
    "    avg_rating = movie_ratings.mean()\n",
    "    rating_count = len(movie_ratings)\n",
    "    \n",
    "    plt.title(f'{movie_title[:25]}...\\\\nAvg: {avg_rating:.2f}, Count: {rating_count}', \n",
    "              fontsize=8, fontweight='bold')\n",
    "    plt.xlabel('Rating')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(np.arange(0.5, 6, 0.5))\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Rating Distributions of 10 Random Movies', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Random movies selected for analysis:\")\n",
    "for i, movie_id in enumerate(random_movies, 1):\n",
    "    movie_title = movies[movies['movieId'] == movie_id]['title'].iloc[0]\n",
    "    movie_ratings = ratings[ratings['movieId'] == movie_id]['rating']\n",
    "    avg_rating = movie_ratings.mean()\n",
    "    rating_count = len(movie_ratings)\n",
    "    print(f\"{i:2d}. {movie_title} (ID: {movie_id}) - Avg: {avg_rating:.3f}, Count: {rating_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l) Plot: Rating distributions of 3 movies that you have watched\n",
    "\n",
    "# I'll select 3 well-known popular movies that I'm familiar with\n",
    "# Let's find some popular movies by searching for well-known titles\n",
    "familiar_movies = []\n",
    "\n",
    "# Search for some well-known movies\n",
    "movie_search_terms = ['Toy Story', 'Forrest Gump', 'The Matrix']\n",
    "for term in movie_search_terms:\n",
    "    matching_movies = movies[movies['title'].str.contains(term, case=False, na=False)]\n",
    "    if not matching_movies.empty:\n",
    "        # Get the first match and check if it has sufficient ratings\n",
    "        movie_id = matching_movies.iloc[0]['movieId']\n",
    "        if movie_id in movie_stats[movie_stats['rating_count'] >= 50]['movieId'].values:\n",
    "            familiar_movies.append(movie_id)\n",
    "\n",
    "# If we don't have enough movies, add some popular ones\n",
    "if len(familiar_movies) < 3:\n",
    "    # Get some of the most rated movies as familiar ones\n",
    "    popular_movies = movie_stats.sort_values('rating_count', ascending=False).head(10)['movieId'].tolist()\n",
    "    for movie_id in popular_movies:\n",
    "        if movie_id not in familiar_movies:\n",
    "            familiar_movies.append(movie_id)\n",
    "        if len(familiar_movies) >= 3:\n",
    "            break\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "for i, movie_id in enumerate(familiar_movies[:3], 1):\n",
    "    plt.subplot(1, 3, i)\n",
    "    \n",
    "    # Get ratings for this movie\n",
    "    movie_ratings = ratings[ratings['movieId'] == movie_id]['rating']\n",
    "    \n",
    "    # Create histogram\n",
    "    plt.hist(movie_ratings, bins=np.arange(0.5, 6, 0.5), alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "    \n",
    "    # Get movie title and stats\n",
    "    movie_title = movies[movies['movieId'] == movie_id]['title'].iloc[0]\n",
    "    movie_genres = movies[movies['movieId'] == movie_id]['genres'].iloc[0]\n",
    "    avg_rating = movie_ratings.mean()\n",
    "    rating_count = len(movie_ratings)\n",
    "    \n",
    "    plt.title(f'{movie_title}\\\\nGenres: {movie_genres}\\\\nAvg: {avg_rating:.2f}, Count: {rating_count}', \n",
    "              fontsize=10, fontweight='bold')\n",
    "    plt.xlabel('Rating')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(np.arange(0.5, 6, 0.5))\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Rating Distributions of 3 Popular Movies', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Movies selected for analysis:\")\n",
    "for i, movie_id in enumerate(familiar_movies[:3], 1):\n",
    "    movie_title = movies[movies['movieId'] == movie_id]['title'].iloc[0]\n",
    "    movie_genres = movies[movies['movieId'] == movie_id]['genres'].iloc[0]\n",
    "    movie_ratings = ratings[ratings['movieId'] == movie_id]['rating']\n",
    "    avg_rating = movie_ratings.mean()\n",
    "    rating_count = len(movie_ratings)\n",
    "    std_rating = movie_ratings.std()\n",
    "    print(f\"{i}. {movie_title}\")\n",
    "    print(f\"   Genres: {movie_genres}\")\n",
    "    print(f\"   Average Rating: {avg_rating:.3f}\")\n",
    "    print(f\"   Rating Count: {rating_count}\")\n",
    "    print(f\"   Standard Deviation: {std_rating:.3f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m) Plot: How many users give which average rating\n",
    "\n",
    "# Calculate average rating per user\n",
    "user_avg_ratings = ratings.groupby('userId')['rating'].agg(['mean', 'count']).reset_index()\n",
    "user_avg_ratings.columns = ['userId', 'avg_rating', 'rating_count']\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot 1: Histogram of average ratings per user\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(user_avg_ratings['avg_rating'], bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "plt.xlabel('Average Rating Given by User')\n",
    "plt.ylabel('Number of Users')\n",
    "plt.title('Distribution of Average Ratings per User')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Scatter plot of average rating vs number of ratings\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(user_avg_ratings['rating_count'], user_avg_ratings['avg_rating'], \n",
    "           alpha=0.6, color='purple', s=20)\n",
    "plt.xlabel('Number of Ratings Given')\n",
    "plt.ylabel('Average Rating Given')\n",
    "plt.title('Average Rating vs Number of Ratings per User')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"User average rating statistics:\")\n",
    "print(f\"• Min average rating: {user_avg_ratings['avg_rating'].min():.3f}\")\n",
    "print(f\"• Max average rating: {user_avg_ratings['avg_rating'].max():.3f}\")\n",
    "print(f\"• Mean average rating: {user_avg_ratings['avg_rating'].mean():.3f}\")\n",
    "print(f\"• Median average rating: {user_avg_ratings['avg_rating'].median():.3f}\")\n",
    "print(f\"• Std average rating: {user_avg_ratings['avg_rating'].std():.3f}\")\n",
    "\n",
    "print(f\"\\nUsers with highest average ratings:\")\n",
    "top_raters = user_avg_ratings.nlargest(10, 'avg_rating')\n",
    "for idx, row in top_raters.iterrows():\n",
    "    print(f\"• User {row['userId']}: {row['avg_rating']:.3f} (from {row['rating_count']} ratings)\")\n",
    "\n",
    "print(f\"\\nUsers with lowest average ratings:\")\n",
    "low_raters = user_avg_ratings.nsmallest(10, 'avg_rating')\n",
    "for idx, row in low_raters.iterrows():\n",
    "    print(f\"• User {row['userId']}: {row['avg_rating']:.3f} (from {row['rating_count']} ratings)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n) Plot: How often a movie was rated as a function of average rating\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot 1: Scatter plot\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.scatter(movie_stats['avg_rating'], movie_stats['rating_count'], \n",
    "           alpha=0.6, color='orange', s=20)\n",
    "plt.xlabel('Average Rating of Movie')\n",
    "plt.ylabel('Number of Ratings Received')\n",
    "plt.title('Number of Ratings vs Average Rating per Movie')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add trend line\n",
    "z = np.polyfit(movie_stats['avg_rating'], movie_stats['rating_count'], 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(movie_stats['avg_rating'], p(movie_stats['avg_rating']), \"r--\", alpha=0.8, linewidth=2)\n",
    "\n",
    "# Plot 2: Box plot by rating bins\n",
    "plt.subplot(2, 1, 2)\n",
    "\n",
    "# Create rating bins\n",
    "movie_stats['rating_bin'] = pd.cut(movie_stats['avg_rating'], \n",
    "                                   bins=[0, 1.5, 2.5, 3.5, 4.5, 5.0], \n",
    "                                   labels=['0-1.5', '1.5-2.5', '2.5-3.5', '3.5-4.5', '4.5-5.0'])\n",
    "\n",
    "# Create box plot\n",
    "rating_bins = []\n",
    "rating_counts_by_bin = []\n",
    "for bin_label in movie_stats['rating_bin'].cat.categories:\n",
    "    bin_data = movie_stats[movie_stats['rating_bin'] == bin_label]['rating_count']\n",
    "    if len(bin_data) > 0:\n",
    "        rating_bins.append(bin_label)\n",
    "        rating_counts_by_bin.append(bin_data)\n",
    "\n",
    "plt.boxplot(rating_counts_by_bin, labels=rating_bins)\n",
    "plt.xlabel('Average Rating Range')\n",
    "plt.ylabel('Number of Ratings')\n",
    "plt.title('Distribution of Rating Counts by Average Rating Range')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Movie rating analysis:\")\n",
    "print(f\"• Correlation between average rating and number of ratings: {movie_stats['avg_rating'].corr(movie_stats['rating_count']):.3f}\")\n",
    "\n",
    "print(f\"\\nStatistics by rating range:\")\n",
    "for bin_label in movie_stats['rating_bin'].cat.categories:\n",
    "    bin_data = movie_stats[movie_stats['rating_bin'] == bin_label]\n",
    "    if len(bin_data) > 0:\n",
    "        print(f\"• {bin_label}: {len(bin_data)} movies, avg ratings count: {bin_data['rating_count'].mean():.1f}\")\n",
    "\n",
    "print(f\"\\nTop 5 movies by average rating:\")\n",
    "top_rated = movie_stats.nlargest(5, 'avg_rating')\n",
    "for idx, row in top_rated.iterrows():\n",
    "    print(f\"• {row['title']}: {row['avg_rating']:.3f} (from {row['rating_count']} ratings)\")\n",
    "\n",
    "print(f\"\\nMost rated movies:\")\n",
    "most_rated = movie_stats.nlargest(5, 'rating_count')\n",
    "for idx, row in most_rated.iterrows():\n",
    "    print(f\"• {row['title']}: {row['rating_count']} ratings (avg: {row['avg_rating']:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# o) Plot: A heatmap of the User Item Matrix\n",
    "\n",
    "# For visualization purposes, we'll create a smaller subset of the matrix\n",
    "# Let's take the first 50 users and first 100 movies for better visualization\n",
    "subset_users = sorted(ratings['userId'].unique())[:50]\n",
    "subset_movies = sorted(ratings['movieId'].unique())[:100]\n",
    "\n",
    "# Create subset of ratings\n",
    "subset_ratings = ratings[\n",
    "    (ratings['userId'].isin(subset_users)) & \n",
    "    (ratings['movieId'].isin(subset_movies))\n",
    "]\n",
    "\n",
    "# Create user-item matrix for the subset\n",
    "subset_matrix = subset_ratings.pivot_table(index='userId', columns='movieId', values='rating')\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.heatmap(subset_matrix, cmap='viridis', cbar=True, \n",
    "            xticklabels=False, yticklabels=False, \n",
    "            cbar_kws={'label': 'Rating'})\n",
    "plt.title('User-Item Matrix Heatmap (First 50 Users, First 100 Movies)', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Movies (Movie IDs)')\n",
    "plt.ylabel('Users (User IDs)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"User-Item Matrix Heatmap Analysis:\")\n",
    "print(f\"• Subset size: {subset_matrix.shape[0]} users × {subset_matrix.shape[1]} movies\")\n",
    "print(f\"• Total possible ratings in subset: {subset_matrix.shape[0] * subset_matrix.shape[1]:,}\")\n",
    "print(f\"• Actual ratings in subset: {subset_matrix.notna().sum().sum():,}\")\n",
    "print(f\"• Sparsity of subset: {1 - (subset_matrix.notna().sum().sum() / (subset_matrix.shape[0] * subset_matrix.shape[1])):.3f}\")\n",
    "\n",
    "# Show some statistics about the subset\n",
    "print(f\"\\nSubset statistics:\")\n",
    "print(f\"• Average rating: {subset_matrix.mean().mean():.3f}\")\n",
    "print(f\"• Rating range: {subset_matrix.min().min():.1f} - {subset_matrix.max().max():.1f}\")\n",
    "print(f\"• Users with most ratings in subset: {subset_matrix.notna().sum(axis=1).max()}\")\n",
    "print(f\"• Movies with most ratings in subset: {subset_matrix.notna().sum(axis=0).max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p) Plot: A heatmap of the User Item Matrix for the 100 most rated movies for the 50 users with most ratings\n",
    "\n",
    "# Get top 50 users with most ratings\n",
    "top_users = ratings.groupby('userId').size().nlargest(50).index.tolist()\n",
    "\n",
    "# Get top 100 movies with most ratings\n",
    "top_movies = ratings.groupby('movieId').size().nlargest(100).index.tolist()\n",
    "\n",
    "# Create subset of ratings for top users and movies\n",
    "top_subset_ratings = ratings[\n",
    "    (ratings['userId'].isin(top_users)) & \n",
    "    (ratings['movieId'].isin(top_movies))\n",
    "]\n",
    "\n",
    "# Create user-item matrix for the top subset\n",
    "top_subset_matrix = top_subset_ratings.pivot_table(index='userId', columns='movieId', values='rating')\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.heatmap(top_subset_matrix, cmap='plasma', cbar=True, \n",
    "            xticklabels=False, yticklabels=False, \n",
    "            cbar_kws={'label': 'Rating'})\n",
    "plt.title('User-Item Matrix Heatmap (Top 50 Users by Rating Count, Top 100 Movies by Rating Count)', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Top 100 Movies by Rating Count')\n",
    "plt.ylabel('Top 50 Users by Rating Count')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Top Users and Movies Matrix Analysis:\")\n",
    "print(f\"• Matrix size: {top_subset_matrix.shape[0]} users × {top_subset_matrix.shape[1]} movies\")\n",
    "print(f\"• Total possible ratings in matrix: {top_subset_matrix.shape[0] * top_subset_matrix.shape[1]:,}\")\n",
    "print(f\"• Actual ratings in matrix: {top_subset_matrix.notna().sum().sum():,}\")\n",
    "print(f\"• Density of matrix: {(top_subset_matrix.notna().sum().sum() / (top_subset_matrix.shape[0] * top_subset_matrix.shape[1])):.3f}\")\n",
    "\n",
    "# Show some statistics about the top subset\n",
    "print(f\"\\nTop subset statistics:\")\n",
    "print(f\"• Average rating: {top_subset_matrix.mean().mean():.3f}\")\n",
    "print(f\"• Rating range: {top_subset_matrix.min().min():.1f} - {top_subset_matrix.max().max():.1f}\")\n",
    "\n",
    "print(f\"\\nTop 10 users by rating count:\")\n",
    "user_counts = ratings.groupby('userId').size().nlargest(10)\n",
    "for user_id, count in user_counts.items():\n",
    "    print(f\"• User {user_id}: {count} ratings\")\n",
    "\n",
    "print(f\"\\nTop 10 movies by rating count:\")\n",
    "movie_counts = ratings.groupby('movieId').size().nlargest(10)\n",
    "for movie_id, count in movie_counts.items():\n",
    "    movie_title = movies[movies['movieId'] == movie_id]['title'].iloc[0]\n",
    "    print(f\"• Movie {movie_id} ({movie_title}): {count} ratings\")\n",
    "\n",
    "print(f\"\\nComparison with full matrix:\")\n",
    "print(f\"• Full matrix sparsity: {sparsity:.4f}\")\n",
    "print(f\"• Top subset sparsity: {1 - (top_subset_matrix.notna().sum().sum() / (top_subset_matrix.shape[0] * top_subset_matrix.shape[1])):.4f}\")\n",
    "print(f\"• The top users and movies matrix is {(1 - (top_subset_matrix.notna().sum().sum() / (top_subset_matrix.shape[0] * top_subset_matrix.shape[1]))) / sparsity:.2f}x denser than the full matrix\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 - Building a baseline RS (7 points)\n",
    "In this exercise we will build a baseline RS and functions to calculate fundamental performance metrics. \n",
    "\n",
    "Build the following baseline RS to predict Top-N (default N=20):\n",
    "1. In reference to the book *Collaborative Filtering Recommender Systems by Michael D. Ekstrand, John T. Riedl and Joseph A. Konstan* (p. 91ff) implement the baseline predictor $$ b_{u,i}= \\mu +b_u +b_i $$ with the regularized user and item average offsets: $$ b_u = \\frac{1}{|I_u| + \\beta_u} \\sum_{i \\in I_u} (r_{u,i} - \\mu) $$ and $$ b_i = \\frac{1}{|U_i| + \\beta_i} \\sum_{u \\in U_i} (r_{u,i} - b_u - \\mu) . $$ Build a recommender system upon this baseline predictor. Set the default damping factors $\\beta_u$ and $\\beta_i$ both to 20.\n",
    "2. Build a RS that recommends based on *random* recommendations.  \n",
    "\n",
    "Output the recommendations for three example users (Ids 1, 3 and 7) and the default parameters. Give the titles of the recommended movies and their predicted scores not just their Ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2 - Building a baseline RS\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "class BaselineRecommender:\n",
    "    \"\"\"\n",
    "    Baseline recommender system implementing the regularized baseline predictor:\n",
    "    b_{u,i} = μ + b_u + b_i\n",
    "    \n",
    "    Where:\n",
    "    b_u = (1 / (|I_u| + β_u)) * Σ(r_{u,i} - μ)\n",
    "    b_i = (1 / (|U_i| + β_i)) * Σ(r_{u,i} - b_u - μ)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, beta_u=20, beta_i=20):\n",
    "        self.beta_u = beta_u\n",
    "        self.beta_i = beta_i\n",
    "        self.mu = 0  # Global average rating\n",
    "        self.b_u = {}  # User biases\n",
    "        self.b_i = {}  # Item biases\n",
    "        self.trained = False\n",
    "        \n",
    "    def fit(self, ratings_df):\n",
    "        \"\"\"Train the baseline predictor on the ratings data\"\"\"\n",
    "        print(\"Training baseline predictor...\")\n",
    "        \n",
    "        # Calculate global average\n",
    "        self.mu = ratings_df['rating'].mean()\n",
    "        print(f\"Global average rating (μ): {self.mu:.3f}\")\n",
    "        \n",
    "        # Initialize biases\n",
    "        user_ratings = defaultdict(list)\n",
    "        item_ratings = defaultdict(list)\n",
    "        \n",
    "        # Group ratings by user and item\n",
    "        for _, row in ratings_df.iterrows():\n",
    "            user_ratings[row['userId']].append(row['rating'])\n",
    "            item_ratings[row['movieId']].append((row['userId'], row['rating']))\n",
    "        \n",
    "        # Calculate user biases\n",
    "        print(\"Calculating user biases...\")\n",
    "        for user_id, ratings_list in user_ratings.items():\n",
    "            numerator = sum(r - self.mu for r in ratings_list)\n",
    "            denominator = len(ratings_list) + self.beta_u\n",
    "            self.b_u[user_id] = numerator / denominator\n",
    "        \n",
    "        # Calculate item biases (iteratively)\n",
    "        print(\"Calculating item biases...\")\n",
    "        # First pass: calculate item biases without user bias correction\n",
    "        for item_id, rating_pairs in item_ratings.items():\n",
    "            numerator = sum(r - self.mu for _, r in rating_pairs)\n",
    "            denominator = len(rating_pairs) + self.beta_i\n",
    "            self.b_i[item_id] = numerator / denominator\n",
    "        \n",
    "        # Second pass: refine item biases with user bias correction\n",
    "        for item_id, rating_pairs in item_ratings.items():\n",
    "            numerator = sum(r - self.b_u.get(user_id, 0) - self.mu for user_id, r in rating_pairs)\n",
    "            denominator = len(rating_pairs) + self.beta_i\n",
    "            self.b_i[item_id] = numerator / denominator\n",
    "        \n",
    "        self.trained = True\n",
    "        print(f\"Training completed. Calculated biases for {len(self.b_u)} users and {len(self.b_i)} items.\")\n",
    "        \n",
    "    def predict(self, user_id, item_id):\n",
    "        \"\"\"Predict rating for a user-item pair\"\"\"\n",
    "        if not self.trained:\n",
    "            raise ValueError(\"Model must be trained before making predictions\")\n",
    "        \n",
    "        b_u = self.b_u.get(user_id, 0)\n",
    "        b_i = self.b_i.get(item_id, 0)\n",
    "        \n",
    "        prediction = self.mu + b_u + b_i\n",
    "        \n",
    "        # Clip to valid rating range\n",
    "        return max(0.5, min(5.0, prediction))\n",
    "    \n",
    "    def recommend_top_n(self, user_id, n=20, exclude_rated=True):\n",
    "        \"\"\"Recommend top N items for a user\"\"\"\n",
    "        if not self.trained:\n",
    "            raise ValueError(\"Model must be trained before making recommendations\")\n",
    "        \n",
    "        # Get all items\n",
    "        all_items = set(self.b_i.keys())\n",
    "        \n",
    "        # Exclude items already rated by user if requested\n",
    "        if exclude_rated:\n",
    "            rated_items = set(ratings[ratings['userId'] == user_id]['movieId'])\n",
    "            candidate_items = all_items - rated_items\n",
    "        else:\n",
    "            candidate_items = all_items\n",
    "        \n",
    "        # Calculate predictions for all candidate items\n",
    "        predictions = []\n",
    "        for item_id in candidate_items:\n",
    "            pred_rating = self.predict(user_id, item_id)\n",
    "            predictions.append((item_id, pred_rating))\n",
    "        \n",
    "        # Sort by predicted rating and return top N\n",
    "        predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "        return predictions[:n]\n",
    "    \n",
    "    def get_bias_stats(self):\n",
    "        \"\"\"Get statistics about the calculated biases\"\"\"\n",
    "        if not self.trained:\n",
    "            return {}\n",
    "        \n",
    "        user_biases = list(self.b_u.values())\n",
    "        item_biases = list(self.b_i.values())\n",
    "        \n",
    "        return {\n",
    "            'global_avg': self.mu,\n",
    "            'user_bias_stats': {\n",
    "                'mean': np.mean(user_biases),\n",
    "                'std': np.std(user_biases),\n",
    "                'min': np.min(user_biases),\n",
    "                'max': np.max(user_biases)\n",
    "            },\n",
    "            'item_bias_stats': {\n",
    "                'mean': np.mean(item_biases),\n",
    "                'std': np.std(item_biases),\n",
    "                'min': np.min(item_biases),\n",
    "                'max': np.max(item_biases)\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Initialize and train the baseline recommender\n",
    "baseline_rec = BaselineRecommender(beta_u=20, beta_i=20)\n",
    "baseline_rec.fit(ratings)\n",
    "\n",
    "# Display bias statistics\n",
    "bias_stats = baseline_rec.get_bias_stats()\n",
    "print(\"\\n=== BASELINE PREDICTOR STATISTICS ===\")\n",
    "print(f\"Global average rating: {bias_stats['global_avg']:.3f}\")\n",
    "print(f\"User bias statistics:\")\n",
    "for stat, value in bias_stats['user_bias_stats'].items():\n",
    "    print(f\"  {stat}: {value:.3f}\")\n",
    "print(f\"Item bias statistics:\")\n",
    "for stat, value in bias_stats['item_bias_stats'].items():\n",
    "    print(f\"  {stat}: {value:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Build a RS that recommends based on random recommendations\n",
    "\n",
    "class RandomRecommender:\n",
    "    \"\"\"\n",
    "    Random recommender system that recommends items randomly\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, random_seed=42):\n",
    "        self.random_seed = random_seed\n",
    "        np.random.seed(random_seed)\n",
    "        self.trained = False\n",
    "        \n",
    "    def fit(self, ratings_df, movies_df):\n",
    "        \"\"\"Initialize the random recommender with available items\"\"\"\n",
    "        print(\"Initializing random recommender...\")\n",
    "        self.movies_df = movies_df\n",
    "        self.available_items = set(movies_df['movieId'].unique())\n",
    "        self.trained = True\n",
    "        print(f\"Random recommender initialized with {len(self.available_items)} items.\")\n",
    "        \n",
    "    def recommend_top_n(self, user_id, n=20, exclude_rated=True):\n",
    "        \"\"\"Recommend N random items for a user\"\"\"\n",
    "        if not self.trained:\n",
    "            raise ValueError(\"Model must be trained before making recommendations\")\n",
    "        \n",
    "        # Exclude items already rated by user if requested\n",
    "        if exclude_rated:\n",
    "            rated_items = set(ratings[ratings['userId'] == user_id]['movieId'])\n",
    "            candidate_items = list(self.available_items - rated_items)\n",
    "        else:\n",
    "            candidate_items = list(self.available_items)\n",
    "        \n",
    "        # Randomly sample N items\n",
    "        if len(candidate_items) >= n:\n",
    "            selected_items = np.random.choice(candidate_items, size=n, replace=False)\n",
    "        else:\n",
    "            selected_items = candidate_items\n",
    "        \n",
    "        # Assign random scores (for consistency with other recommenders)\n",
    "        random_scores = np.random.uniform(1.0, 5.0, len(selected_items))\n",
    "        \n",
    "        # Create list of (item_id, score) tuples\n",
    "        recommendations = list(zip(selected_items, random_scores))\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "# Initialize and train the random recommender\n",
    "random_rec = RandomRecommender(random_seed=42)\n",
    "random_rec.fit(ratings, movies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output recommendations for three example users (IDs 1, 3 and 7)\n",
    "\n",
    "def display_recommendations(recommender, user_id, n=20, recommender_name=\"Recommender\"):\n",
    "    \"\"\"Display recommendations for a user with movie titles and scores\"\"\"\n",
    "    print(f\"\\n=== {recommender_name.upper()} - TOP {n} RECOMMENDATIONS FOR USER {user_id} ===\")\n",
    "    \n",
    "    # Get recommendations\n",
    "    recommendations = recommender.recommend_top_n(user_id, n=n, exclude_rated=True)\n",
    "    \n",
    "    if not recommendations:\n",
    "        print(\"No recommendations available (user may have rated all items)\")\n",
    "        return\n",
    "    \n",
    "    # Display recommendations with movie titles\n",
    "    for i, (movie_id, score) in enumerate(recommendations, 1):\n",
    "        movie_title = movies[movies['movieId'] == movie_id]['title'].iloc[0]\n",
    "        movie_genres = movies[movies['movieId'] == movie_id]['genres'].iloc[0]\n",
    "        print(f\"{i:2d}. {movie_title}\")\n",
    "        print(f\"    Movie ID: {movie_id}, Predicted Score: {score:.3f}\")\n",
    "        print(f\"    Genres: {movie_genres}\")\n",
    "        print()\n",
    "    \n",
    "    print(f\"Total recommendations: {len(recommendations)}\")\n",
    "\n",
    "# Test users\n",
    "test_users = [1, 3, 7]\n",
    "\n",
    "# Display recommendations for each user with both recommenders\n",
    "for user_id in test_users:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"RECOMMENDATIONS FOR USER {user_id}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Check if user exists in the dataset\n",
    "    if user_id not in ratings['userId'].unique():\n",
    "        print(f\"User {user_id} not found in the dataset!\")\n",
    "        continue\n",
    "    \n",
    "    user_rating_count = len(ratings[ratings['userId'] == user_id])\n",
    "    user_avg_rating = ratings[ratings['userId'] == user_id]['rating'].mean()\n",
    "    print(f\"User {user_id} has rated {user_rating_count} movies with average rating: {user_avg_rating:.3f}\")\n",
    "    \n",
    "    # Baseline recommender recommendations\n",
    "    display_recommendations(baseline_rec, user_id, n=20, recommender_name=\"Baseline\")\n",
    "    \n",
    "    # Random recommender recommendations\n",
    "    display_recommendations(random_rec, user_id, n=20, recommender_name=\"Random\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 Summary\n",
    "\n",
    "**Exercise 2 - Building a baseline RS** has been completed successfully!\n",
    "\n",
    "### Implemented Components:\n",
    "\n",
    "1. **Baseline Predictor with Regularized Offsets**: \n",
    "   - Implemented the formula: `b_{u,i} = μ + b_u + b_i`\n",
    "   - User bias: `b_u = (1/(|I_u| + β_u)) * Σ(r_{u,i} - μ)`\n",
    "   - Item bias: `b_i = (1/(|U_i| + β_i)) * Σ(r_{u,i} - b_u - μ)`\n",
    "   - Default regularization parameters: β_u = 20, β_i = 20\n",
    "\n",
    "2. **Baseline Recommender System**:\n",
    "   - Trained on the full ratings dataset\n",
    "   - Calculates user and item biases with regularization\n",
    "   - Provides Top-N recommendations (default N=20)\n",
    "   - Excludes already rated items from recommendations\n",
    "\n",
    "3. **Random Recommender System**:\n",
    "   - Recommends items randomly from the available movie catalog\n",
    "   - Assigns random scores for consistency\n",
    "   - Uses reproducible random seed (42)\n",
    "\n",
    "4. **Recommendation Output**:\n",
    "   - Generated Top-20 recommendations for users 1, 3, and 7\n",
    "   - Displayed movie titles, IDs, predicted scores, and genres\n",
    "   - Compared baseline vs random recommendations\n",
    "\n",
    "### Key Features:\n",
    "- **Modular Design**: Clean class-based implementation for both recommenders\n",
    "- **Comprehensive Output**: Movie titles, scores, and metadata for all recommendations\n",
    "- **Performance Analysis**: Statistical comparison between baseline and random approaches\n",
    "- **Proper Regularization**: Prevents overfitting with β parameters\n",
    "- **User-Friendly**: Clear display of recommendations with full movie information\n",
    "\n",
    "The baseline recommender provides a much more sophisticated approach than random recommendations, using learned user and item biases to make personalized predictions. This serves as a strong foundation for comparison with more advanced collaborative filtering methods in subsequent exercises.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional analysis: Compare baseline and random recommenders\n",
    "\n",
    "print(\"=== COMPARISON ANALYSIS ===\")\n",
    "\n",
    "# Calculate some basic statistics for comparison\n",
    "def analyze_recommender_performance(recommender, recommender_name, sample_users=None):\n",
    "    \"\"\"Analyze basic performance metrics for a recommender\"\"\"\n",
    "    if sample_users is None:\n",
    "        sample_users = [1, 3, 7]\n",
    "    \n",
    "    total_recommendations = 0\n",
    "    avg_scores = []\n",
    "    \n",
    "    for user_id in sample_users:\n",
    "        if user_id in ratings['userId'].unique():\n",
    "            recommendations = recommender.recommend_top_n(user_id, n=20, exclude_rated=True)\n",
    "            total_recommendations += len(recommendations)\n",
    "            if recommendations:\n",
    "                scores = [score for _, score in recommendations]\n",
    "                avg_scores.extend(scores)\n",
    "    \n",
    "    if avg_scores:\n",
    "        return {\n",
    "            'name': recommender_name,\n",
    "            'total_recommendations': total_recommendations,\n",
    "            'avg_predicted_score': np.mean(avg_scores),\n",
    "            'std_predicted_score': np.std(avg_scores),\n",
    "            'min_predicted_score': np.min(avg_scores),\n",
    "            'max_predicted_score': np.max(avg_scores)\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            'name': recommender_name,\n",
    "            'total_recommendations': 0,\n",
    "            'avg_predicted_score': 0,\n",
    "            'std_predicted_score': 0,\n",
    "            'min_predicted_score': 0,\n",
    "            'max_predicted_score': 0\n",
    "        }\n",
    "\n",
    "# Analyze both recommenders\n",
    "baseline_stats = analyze_recommender_performance(baseline_rec, \"Baseline\")\n",
    "random_stats = analyze_recommender_performance(random_rec, \"Random\")\n",
    "\n",
    "print(\"Performance Comparison:\")\n",
    "print(f\"{'Metric':<25} {'Baseline':<15} {'Random':<15}\")\n",
    "print(\"-\" * 55)\n",
    "print(f\"{'Total Recommendations':<25} {baseline_stats['total_recommendations']:<15} {random_stats['total_recommendations']:<15}\")\n",
    "print(f\"{'Avg Predicted Score':<25} {baseline_stats['avg_predicted_score']:.3f}{'':<11} {random_stats['avg_predicted_score']:.3f}{'':<11}\")\n",
    "print(f\"{'Std Predicted Score':<25} {baseline_stats['std_predicted_score']:.3f}{'':<11} {random_stats['std_predicted_score']:.3f}{'':<11}\")\n",
    "print(f\"{'Min Predicted Score':<25} {baseline_stats['min_predicted_score']:.3f}{'':<11} {random_stats['min_predicted_score']:.3f}{'':<11}\")\n",
    "print(f\"{'Max Predicted Score':<25} {baseline_stats['max_predicted_score']:.3f}{'':<11} {random_stats['max_predicted_score']:.3f}{'':<11}\")\n",
    "\n",
    "print(f\"\\n=== BASELINE PREDICTOR DETAILS ===\")\n",
    "print(f\"Global average rating (μ): {baseline_rec.mu:.3f}\")\n",
    "print(f\"Regularization parameters: β_u = {baseline_rec.beta_u}, β_i = {baseline_rec.beta_i}\")\n",
    "print(f\"Number of users with biases: {len(baseline_rec.b_u)}\")\n",
    "print(f\"Number of items with biases: {len(baseline_rec.b_i)}\")\n",
    "\n",
    "print(f\"\\n=== RANDOM RECOMMENDER DETAILS ===\")\n",
    "print(f\"Random seed: {random_rec.random_seed}\")\n",
    "print(f\"Available items for recommendation: {len(random_rec.available_items)}\")\n",
    "\n",
    "print(f\"\\nNote: The baseline recommender uses the regularized baseline predictor formula:\")\n",
    "print(f\"b_{{u,i}} = μ + b_u + b_i\")\n",
    "print(f\"where b_u and b_i are regularized user and item biases respectively.\")\n",
    "print(f\"This provides a more sophisticated baseline than random recommendations.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 - Evaluation methods (15 points)\n",
    "Split the data into train/validation set and a separate test set. This test set shall contain the first 20% of the users and shall not be used at all before exercise 10. With the remaining 80% do the following: \n",
    "Implement a function to partition your dataset for an offline evaluation based on holding out of random users with 5x cross validation with a 80/20 train/validation split. Within the validation set implement a masking with *all but n* approach. \n",
    "See page 2942 of https://jmlr.csail.mit.edu/papers/volume10/gunawardana09a/gunawardana09a.pdf for details on this approach. \n",
    "\n",
    "Choose the number of masked items n reasonably and explain your considerations.\n",
    "\n",
    "Implement functions to calculate the following metrics:\n",
    "- *Mean Absolute Error (MAE)* \n",
    "- *Root Mean Square Error (RMSE)*\n",
    "- *Precision@N* with default $N=20$ and relevance threshold 4.0 stars.\n",
    "- *Recall@N* with default $N=20$ and relevance threshold 4.0 stars.\n",
    "- *One metric of the following: Novelty, Diverstity, Unexpectedness, Serendipity, Coverage*\n",
    "Explain each of these. How does the relevance threshold influence the metrics? How would you choose this parameter?\n",
    "\n",
    "Note: For *precision@N* and *Recall@N* use the definitions from https://medium.com/@m_n_malaeb/recall-and-precision-at-k-for-recommender-systems-618483226c54 with one exception: In case of the denominator being zero, set the metric to 0. \n",
    "\n",
    "For *Novelty*, *Diverstity*, *Unexpectedness*, *Serendipity*, *Coverage* you may use definitions from Silveira et al. https://link.springer.com/article/10.1007/s13042-017-0762-9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 - Optimize hyperparameters of baseline RS (6 points)\n",
    "Optimize the hyperparameters $\\beta_u$ and $\\beta_i$ for the baseline RS from exercise 2 based on the RMSE metric. To save computation time find a reasonable maximum value for the betas. Explain your approach and your solution.\n",
    "Plot the MAE, RMSE, Precision@N, Recall@N as functions of the betas.\n",
    "\n",
    "Which metric would you use for hyperparameter tuning? Explain your decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5 - Collaborative filtering; item-based and user-based (12 points)\n",
    "In this exersise we will build several different collaborative-filtering RS based on nearest neighbour technique, both in terms of item and user. \n",
    "\n",
    "Implement:\n",
    "1. a RS based on the $K$ most similar items (K nearest neighbours). Similarity shall be calculated based on *cosine similarity*. \n",
    "2. a RS based on the $K$ most similar items (K nearest neighbours). Similarity shall be calculated based on *Pearson Correlation Coefficienct*. \n",
    "3. a RS based on the $K$ most similar users (K nearest neighbours). Similarity shall be calculated based on *cosine similarity*. \n",
    "4. a RS based on the $K$ most similar users (K nearest neighbours). Similarity shall be calculated based on *Pearson Correlation Coefficienct*. \n",
    "\n",
    "Each should have a default $K$ of 30.\n",
    "\n",
    "Explain how you handle NaN values in the user rating matrix when computing similarities? What other preparations are useful such as normalization and mean centering?\n",
    "\n",
    "Describe the two similarity metrics.\n",
    "\n",
    "Show the top 20 recommended items for user ids 3, 5 and 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Exercise 6 - Optimize hyperparameter $K$ (6 points)\n",
    "Optimize the hyperparameter $K$ for all RS from the prior exercise optimizing for minimal RMSE. \n",
    "For each RS plot RMSE, Precision@N and Recall@N as a function of $K$. \n",
    "\n",
    "Compare the results of these four RS on the 3 example users. Do the results match your expectation? Describe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7 - Model-based RS: SVD (10 points)\n",
    "In this exercise we will use the unsupervised method *singular value decomposition (SVD)* from the python package *surprise* (https://surpriselib.com, documentation https://surprise.readthedocs.io/en/stable/matrix_factorization.html). SVD can compress much of the information of a matrix in few components.  \n",
    "\n",
    "a)Run the SVD RS and show the results on the three example users from exercise 2. Explain how this algorithm works.\n",
    "\n",
    "Note: A very good general introduction to SVD is this youtube video series starting with https://www.youtube.com/watch?v=gXbThCXjZFM&t=337s . See *Collaborative filtering recommender systems* by Ekstrand et al. *Mining of massive datasets* by Leskovec, Kapitel 11 (2020) and, *Recommender systems: The textbook*, by Aggarwal, chapter 3\n",
    "\n",
    "b) We explore now what latent factors SVD has learned. Generate an interactive 2D UMAP plot of the biggest 10 latent movie factors. \n",
    "UMAP is a method for dimensionality reduction. Dimensionality reduction is typically used to respresent high dimensional data sets in less dimensions with goal to allow for visualization. See for the documentation of the python package:\n",
    "https://umap-learn.readthedocs.io/en/latest/ and for interactive experimentation with this method https://pair-code.github.io/understanding-umap/ to gain a intuitive understanding of the two important parameters of this method: n_neighbours and min_dist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7 - Model-based RS: SVD (10 points)\n",
    "In this exercise we will use the unsupervised method *singular value decomposition (SVD)* from the python package *surprise* (https://surpriselib.com, documentation https://surprise.readthedocs.io/en/stable/matrix_factorization.html). SVD can compress much of the information of a matrix in few components.  \n",
    "\n",
    "a)Run the SVD RS and show the results on the three example users from exercise 2. Explain how this algorithm works.\n",
    "\n",
    "Note: A very good general introduction to SVD is this youtube video series starting with https://www.youtube.com/watch?v=gXbThCXjZFM&t=337s . See *Collaborative filtering recommender systems* by Ekstrand et al. *Mining of massive datasets* by Leskovec, Kapitel 11 (2020) and, *Recommender systems: The textbook*, by Aggarwal, chapter 3\n",
    "\n",
    "b) We explore now what latent factors SVD has learned. Generate an interactive 2D UMAP plot of the biggest 10 latent movie factors. Explore the resulting plot. With your movie knowledge can you interpret the movie clusters that form in the plot? Can you give names to (some) clusters?\n",
    "\n",
    "Background: UMAP is a method for dimensionality reduction. Dimensionality reduction is typically used to respresent high dimensional data sets in less dimensions with goal to allow interpretable 2D/3D visualization. See for the documentation of the python package https://umap-learn.readthedocs.io/en/latest/ and for interactive experimentation with this method https://pair-code.github.io/understanding-umap/ to gain an intuition of the two important parameters of this method: *n_neighbours* and *min_dist*.\n",
    "\n",
    "**At the MSP defense I do not expect a mathematical explanation how UMAP works. However you should have a intuition what the methods does and how the two parameters mentioned above influence the results.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8 - Optimize hyperparameter $k$ or `n_factors` (4 points)\n",
    "Optimize the hyperparameter, representing the number of greatest SVD components used for the truncated reconstruction of the user item matrix, to minimize RMSE.\n",
    "Plot RMSE, Precision@N and Recall@N as a function of this hyperparameter. Finally output all performance metrics from exercise 3 for the optimal $k$ value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9 - Everything goes (30 points)\n",
    "In this exercise you can explore different methods of RS. You are not limited what methods you apply. You can try to improve the methods from the earlier exercises by modifiying them or generating ensemble or hybrid RS. Also you could train deep neural networks, use NLP methods, use the available links to imdb available in the dataset to further enrich the dataset or find an obscure method by someone else on Github. \n",
    "Document what your inspirations and sources are and describe the method conceptually. \n",
    "\n",
    "**Build and optimize in total *three* different methods. The last one has the additional requirement that it should increase the diversity of the recommendations in order to minimize filter bubbles.**\n",
    "\n",
    "**Important: If you use the work of someone else you must be able to explain the method conceptually during the defense MSP.** \n",
    "\n",
    "Output the performance metrics of exercise 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10 - Compare all RS that you build in this challenge (8 points)\n",
    "a) Compile a table with the performance metrics of exercise 3 for all RS from this MC (Make sure to include the baseline RS and random RS) on the test set defined in exercise 3. Also generate comparative plots. Discuss.\n",
    "\n",
    "b) Why is it important to keep a test set seperate till the end of a benchmark?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read the Guidelines for Implementation and Submission one more time.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RSY_FS24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
